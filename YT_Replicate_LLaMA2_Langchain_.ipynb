{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RRYSu48huSUW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain tiktoken replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-KFB7J_u_3L",
    "outputId": "11124df2-d791-4f6a-c939-832175e9f807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.305\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dNA4TsHpu6OM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_1tXV1Jxf2JBIiunu0PrGCEuYgOabv251OjQ0x\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqwsGJDhvAQ5"
   },
   "source": [
    "## LLaMA2 with Replicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OwVVX3Ghd86i"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Replicate\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MIx1Q08jePKS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Init param `input` is deprecated, please use `model_kwargs` instead.\n"
     ]
    }
   ],
   "source": [
    "llm = Replicate(\n",
    "    model=\"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n",
    "    input={\"temperature\": 0.75,\n",
    "           \"max_length\": 500,\n",
    "           \"top_p\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ObbNZcnmeZaT",
    "outputId": "9d88653b-45de-4f3f-bdf7-525c28ec22a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a human.\\n\\nMy name is Jake.\\n\\nI am not a machine.\\n\\nI am here to learn.\\n\\nPlease teach me.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Greet me!\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "og7AckcDeaGT",
    "outputId": "27252aee-2241-42d3-de2e-2d4d65ed2516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dogs do not have the ability to operate complex machinery like cars.\n",
      "2. Dogs do not have the physical dexterity or coordination to handle the pedals and steering wheel.\n",
      "3. Dogs do not have the cognitive ability to understand traffic laws and road signs.\n",
      "\n",
      "Therefore, the answer is no, a dog cannot drive a car.\n",
      "\n",
      "Please note that this is a fictional scenario and it is not possible for a dog to drive a car in reality."
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Replicate(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model=\"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n",
    "    input={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1},\n",
    ")\n",
    "prompt = \"\"\"\n",
    "User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?\n",
    "Assistant:\n",
    "\"\"\"\n",
    "_ = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4O_5Q-me7qQ"
   },
   "source": [
    "## LLaMA70B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nV7bdS2e5fe",
    "outputId": "a250b510-6dd7-4397-9ee3-75284bb62d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, a dog cannot drive a car.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. Dogs do not have the physical ability to operate a vehicle, as they do not have opposable thumbs or the necessary strength to manipulate the controls.\n",
      "2. Dogs also lack the cognitive abilities to understand the complex processes involved in driving a car, such as reading traffic signals, navigating roads, and avoiding obstacles.\n",
      "3. Additionally, dogs are not licensed or certified to operate vehicles, and it would be unsafe for both the dog and other road users if they were to attempt to drive a car.\n",
      "\n",
      "Therefore, based on these reasons, it is not possible for a dog to drive a car."
     ]
    }
   ],
   "source": [
    "llm = Replicate(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model=\"replicate/llama70b-v2-chat:2d19859030ff705a87c746f7e96eea03aefb71f166725aee39692f1476566d48\",\n",
    "    input={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1},\n",
    ")\n",
    "prompt = \"\"\"\n",
    "User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?\n",
    "Assistant:\n",
    "\"\"\"\n",
    "_ = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpRsI8Cmij95"
   },
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvxcEvTFfSTc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo-FSysZiVkA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "oTf-vPB8wPGs",
    "outputId": "ac1966b6-041b-45f1-f178-201be292bb8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nWhat is the temperature in Melbourne?[/INST]\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"What is the temperature in Melbourne?\"\n",
    "\n",
    "get_prompt(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "R7n4grsO1UN1",
    "outputId": "736774eb-01dc-4827-b355-e6e0e399e0d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[INST]<<SYS>>\\nYou are an expert and summarization and reducing the number of words used\\n<</SYS>>\\n\\nSummarize the following text for me {text}[/INST]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Summarize the following text for me {text}\"\n",
    "\n",
    "system_prompt = \"You are an expert and summarization and reducing the number of words used\"\n",
    "\n",
    "get_prompt(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fTKdQRCdis7"
   },
   "source": [
    "## LangChain basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP4Bk5YBf1mI"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL7JGQ5iCzIy"
   },
   "outputs": [],
   "source": [
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTBrt6XmuY3Y",
    "outputId": "33967518-f290-4387-bcab-c18968fad02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an advanced assistant that excels at translation. \n",
      "<</SYS>>\n",
      "\n",
      "Convert the following text from English to French:\n",
      "\n",
      " {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an advanced assistant that excels at translation. \"\n",
    "instruction = \"Convert the following text from English to French:\\n\\n {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0fUGp-muY6f",
    "outputId": "5c6261e1-02c7-4e43-fa66-ef2060e0c098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help! The translation of \"how are you today?\" from English to French is:\n",
      "\n",
      "\" Comment allez-vous aujourd'hui?\"\n",
      "\n",
      "I hope that helps! If you have any other questions or need further assistance, feel free to ask. Sure, I'd be happy to help! The translation of \"how are you today?\" from English to French is:  \"\n",
      "Comment allez-vous aujourd'hui?\"  I hope that helps! If you have any other questions or need further\n",
      "assistance, feel free to ask.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"how are you today?\"\n",
    "output = llm_chain.run(text)\n",
    "\n",
    "parse_text(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e38eSqFb3hET"
   },
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dt4mn5CJuY90",
    "outputId": "c5f86d24-de8f-4ac7-ebe4-707f9fdf221e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an expert and summarization and expressing key ideas succintly\n",
      "<</SYS>>\n",
      "\n",
      "Summarize the following article for me {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Summarize the following article for me {text}\"\n",
    "system_prompt = \"You are an expert and summarization and expressing key ideas succintly\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gykQOf4OuZBK",
    "outputId": "6fdf8309-bb3c-4ba1-eda6-c7652d8d7b89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "text = '''Twitter (now X) CEO Linda Yaccarino claims usage at ‘all time high’ in memo to staff\n",
    "Twitter’s (now X’s) newly established CEO Linda Yaccarino touts the company’s success and X’s future plans in a company-wide memo obtained by CNBC. The exec once again claims, without sharing any specific metrics, that the service’s usage is at an “all time high,” and hints at what’s to come in terms of new product experiences for the newly rebranded platform.\n",
    "\n",
    "The service formerly known as Twitter has been working to become more than just a social network and more of an “everything app,” as owner Elon Musk dubbed it.\n",
    "\n",
    "As the Telsa and Space X exec explained in October 2022, telegraphing Twitter’s eventual rebranding, buying Twitter was meant to be “an accelerant to creating X, the everything app.”\n",
    "\n",
    "\n",
    "His grand plan has been to create an app that allows creators to monetize their content, then later moves into payments services and even banking, Musk remarked during a Twitter Spaces livestream with advertisers in November. At the time, he even mentioned the possibility of establishing money market accounts on Twitter that would pay a high-interest rate to attract consumers to X.\n",
    "\n",
    "Those possible product concepts were again referenced in Yaccarino’s new missive, when she writes, “Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.”\n",
    "\n",
    "Twitter, now X, has already implemented some of Musk’s ideas around videos and creator monetization. In May, the company began allowing subscribers to upload two-hour videos to its service, which advertiser Apple then leveraged when it released the entire first episode of its hit Apple TV+ show “Silo” on the platform. Fired Fox News host Tucker Carlson had been posting lengthy videos to Twitter as well, until ordered to stop by the network.\n",
    "\n",
    "In addition, earlier this month, Twitter began sharing ad revenue with verified creators.\n",
    "\n",
    "However, all is not well at Twitter X, whose traffic — at least by third-party measurements — has been dropping. Data from web analytics firm Similarweb indicated Twitter’s web traffic declined 5% for the first two days its latest rival, Instagram Threads, became generally available, compared with the week prior. Plus, Similarweb said Twitter’s web traffic was down 11% compared with the same days in 2022. Additionally, Cloudflare CEO Matthew Prince earlier this month tweeted a graph of traffic to the Twitter.com domain that showed “Twitter traffic tanking,” he said.\n",
    "\n",
    "\n",
    "Yaccarino subtly pushed back at those reports at the time, claiming that Twitter had its largest usage day since February in early July. She did not share any specific metrics or data. At the same time, however, the company was quietly blocking links to Threads.net in Twitter searches, suggesting it was concerned about the new competition.\n",
    "\n",
    "Today, Yaccarino repeats her vague claims around X’s high usage in her company-wide memo even as analysts at Forrester are predicting X will either shut down or be acquired within the next 12 months and numerous critics concur that the X rebrand is destined to fail.\n",
    "\n",
    "Yaccarino’s memo, otherwise, was mostly a lot of cheerleading, applauding X’s team for their work and touting X’s ability to “impress the world all over again,” as Twitter once did.\n",
    "\n",
    "The full memo, courtesy of CBNC, is below:\n",
    "\n",
    "Hi team,\n",
    "\n",
    "What a momentous weekend. As I said yesterday, it’s extremely rare, whether it’s in life or in business, that you have the opportunity to make another big impression. That’s what we’re experiencing together, in real time. Take a moment to put it all into perspective.\n",
    "\n",
    "17 years ago, Twitter made a lasting imprint on the world. The platform changed the speed at which people accessed information. It created a new dynamic for how people communicated, debated, and responded to things happening in the world. Twitter introduced a new way for people, public figures, and brands to build long lasting relationships. In one way or another, everyone here is a driving force in that change. But equally all our users and partners constantly challenged us to dream bigger, to innovate faster, and to fulfill our great potential.\n",
    "\n",
    "With X we will go even further to transform the global town square — and impress the world all over again.\n",
    "\n",
    "Our company uniquely has the drive to make this possible. Many companies say they want to move fast — but we enjoy moving at the speed of light, and when we do, that’s X. At our core, we have an inventor mindset — constantly learning, testing out new approaches, changing to get it right and ultimately succeeding.\n",
    "\n",
    "With X, we serve our entire community of users and customers by working tirelessly to preserve free expression and choice, create limitless interactivity, and create a marketplace that enables the economic success of all its participants.\n",
    "\n",
    "The best news is we’re well underway. Everyone should be proud of the pace of innovation over the last nine months — from long form content, to creator monetization, and tremendous advancements in brand safety protections. Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.\n",
    "\n",
    "Please don’t take this moment for granted. You’re writing history, and there’s no limit to our transformation. And everyone, is invited to build X with us.\n",
    "\n",
    "Elon and I will be working across every team and partner to bring X to the world. That includes keeping our entire community up to date, ensuring that we all have the information we need to move forward.\n",
    "\n",
    "Now, let’s go make that next big impression on the world, together.\n",
    "\n",
    "Linda'''\n",
    "\n",
    "count_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4VDYh3VuZD_",
    "outputId": "a21b2c6d-baa7-419a-9993-6064355e518f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Twitter's CEO Linda Yaccarino claims that the platform's usage is at an all-time high, despite reports of declining traffic. She mentions new product experiences for the recently rebranded platform, including audio, video, messaging, payments, and banking. The company aims to create a global marketplace for ideas, goods, services, and opportunities. Yaccarino also highlights the company's efforts to preserve free expression and choice, create limitless interactivity, and ensure brand safety protections. She encourages the team to continue innovating and invites everyone to build X with them.86\n",
      " Twitter's CEO Linda Yaccarino claims that the platform's usage is at an all-time high, despite\n",
      "reports of declining traffic. She mentions new product experiences for the recently rebranded\n",
      "platform, including audio, video, messaging, payments, and banking. The company aims to create a\n",
      "global marketplace for ideas, goods, services, and opportunities. Yaccarino also highlights the\n",
      "company's efforts to preserve free expression and choice, create limitless interactivity, and ensure\n",
      "brand safety protections. She encourages the team to continue innovating and invites everyone to\n",
      "build X with them.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = llm_chain.run(text)\n",
    "print(count_words(output))\n",
    "parse_text(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsPFAOBK74Yi"
   },
   "source": [
    "## Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Inghmejz_Tkj"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7DAJoLEAOU7",
    "outputId": "a757837a-dcbb-4303-8b00-a4b011cb1151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "{chat_history} \n",
      "\n",
      "User: {user_input} \n",
      "\n",
      "AI:[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input} \\n\\nAI:\"\n",
    "system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv7iPmbI_TnI"
   },
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGjjiqxz_TqG"
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "X3VadjbM_Tsx",
    "outputId": "53ab698c-7c73-4b53-8493-39c2ee33d897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Sam, it's nice to meet you. How can I assist you today?CPU times: user 308 ms, sys: 14.3 ms, total: 323 ms\n",
      "Wall time: 5.53 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hello Sam, it's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"Hi, my name is Sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "ENqnCnun_TvC",
    "outputId": "4747f4d9-a569-4799-bfd1-0f8e33ffd4ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'm just an AI designed to assist with a wide range of tasks, from answering questions to providing information and guidance. I'm here to help you in any way I can, while ensuring that my responses are socially unbiased and positive in nature. Is there something specific you'd like to know or discuss?CPU times: user 288 ms, sys: 15.9 ms, total: 304 ms\n",
      "Wall time: 14.3 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure, I'm just an AI designed to assist with a wide range of tasks, from answering questions to providing information and guidance. I'm here to help you in any way I can, while ensuring that my responses are socially unbiased and positive in nature. Is there something specific you'd like to know or discuss?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"Can you tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "_HIqTiLc_Tx2",
    "outputId": "c23fb3ac-38e3-4320-eee3-52887fccac5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I can help you with that! Today is indeed Friday, and Friday is the fifth day of the week. So, the answer to your question is \"5\".CPU times: user 157 ms, sys: 21.2 ms, total: 178 ms\n",
      "Wall time: 7.96 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Sure, I can help you with that! Today is indeed Friday, and Friday is the fifth day of the week. So, the answer to your question is \"5\".'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"Today is Friday. What number day of the week is that?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "r5Kn6p15Ky2F",
    "outputId": "92a33b0f-4f79-48be-bdbb-2458303048b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! Today is Saturday.CPU times: user 73.4 ms, sys: 5.08 ms, total: 78.5 ms\n",
      "Wall time: 4.08 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello! Today is Saturday.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"what is the day today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "CGXX3RoT74qz",
    "outputId": "c9a4d59c-c422-48fd-a3f3-1fca6cf5cd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm not able to determine your name as I don't have access to personal information. However, I can tell you that it's great to meet you! Is there anything else I can assist you with?CPU times: user 210 ms, sys: 21.6 ms, total: 232 ms\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hello! I'm not able to determine your name as I don't have access to personal information. However, I can tell you that it's great to meet you! Is there anything else I can assist you with?\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "YCBnZHm9BOQU",
    "outputId": "4757d720-7515-4cf5-fcaa-51648be9fa83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! The Olympics are an international multi-sport event held every four years, where athletes from around the world compete in various sports. The Olympic Games have a rich history, dating back to ancient Greece, and have evolved over time to include a wide range of sports and events.\n",
      "\n",
      "The modern Olympic Games were revived in 1896, and have been held every four years since then, except during the two World Wars. The Olympics are organized by the International Olympic Committee (IOC), which is responsible for setting the rules and regulations for the games.\n",
      "\n",
      "The Olympics feature a wide range of sports, including track and field, swimming, gymnastics, basketball, soccer, and many others. Athletes from around the world compete in these events, and medals are awarded to the top three finishers in each event.\n",
      "\n",
      "The Olympics also have a rich tradition of symbolism and pageantry, with the opening and closing ceremonies featuring elaborate performances and displays of national pride. The Olympic flame, which is lit at the start of the games and extinguished at the end, is a symbol of the Olympic spirit and the unity of the participating nations.\n",
      "\n",
      "In addition to the summer Olympics, there are also winter Olympics, which are held every four years and feature sports such as skiing, skating, and curling. The Paralympic Games, which are held immediately following the Olympics, feature athletes with disabilities competing in various sports.\n",
      "\n",
      "Overall, the Olympics are a celebrated event that brings together athletes and spectators from around the world in a celebration of sports, culture, and human achievement.CPU times: user 1.31 s, sys: 103 ms, total: 1.41 s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello! The Olympics are an international multi-sport event held every four years, where athletes from around the world compete in various sports. The Olympic Games have a rich history, dating back to ancient Greece, and have evolved over time to include a wide range of sports and events.\\n\\nThe modern Olympic Games were revived in 1896, and have been held every four years since then, except during the two World Wars. The Olympics are organized by the International Olympic Committee (IOC), which is responsible for setting the rules and regulations for the games.\\n\\nThe Olympics feature a wide range of sports, including track and field, swimming, gymnastics, basketball, soccer, and many others. Athletes from around the world compete in these events, and medals are awarded to the top three finishers in each event.\\n\\nThe Olympics also have a rich tradition of symbolism and pageantry, with the opening and closing ceremonies featuring elaborate performances and displays of national pride. The Olympic flame, which is lit at the start of the games and extinguished at the end, is a symbol of the Olympic spirit and the unity of the participating nations.\\n\\nIn addition to the summer Olympics, there are also winter Olympics, which are held every four years and feature sports such as skiing, skating, and curling. The Paralympic Games, which are held immediately following the Olympics, feature athletes with disabilities competing in various sports.\\n\\nOverall, the Olympics are a celebrated event that brings together athletes and spectators from around the world in a celebration of sports, culture, and human achievement.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"Can you tell me about the olympics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "Nf2QRotKBOTq",
    "outputId": "66c70a7e-0b4c-484d-9daa-4fcd4db82285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I can help you with that! In this chat, we've talked about the following topics:\n",
      "\n",
      "1. Introduction and greeting\n",
      "2. The day of the week (Friday)\n",
      "3. Personal information (name)\n",
      "4. The Olympics\n",
      "5. Recap of the chat history\n",
      "\n",
      "Is there anything else I can help you with?CPU times: user 287 ms, sys: 34.2 ms, total: 321 ms\n",
      "Wall time: 16.3 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure, I can help you with that! In this chat, we've talked about the following topics:\\n\\n1. Introduction and greeting\\n2. The day of the week (Friday)\\n3. Personal information (name)\\n4. The Olympics\\n5. Recap of the chat history\\n\\nIs there anything else I can help you with?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm_chain.predict(user_input=\"What have we talked about in this Chat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4aLRBjEBOW9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
